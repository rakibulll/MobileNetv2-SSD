{
  "metadata": {
    "colab": {
      "name": "Mobilenetv2 + SSD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "accelerator": "GPU",
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Implementation of MobileNetv2+SSD<br>\nThis is an implementation of the MobileNetv2 + SSD architecture for a relatively simpler task of determining bounding boxes for MNIST images embedded in a box. Each box contains only one digit(28x28 MNIST embedded into a 224x224 box) as of now, but the number of predictions per image can be expanded easily (the training outputs need to modified). Also, no data augmentation has been used till now (Colab kept crashing when I increased the dataset size beyond 1000, so the initial amount of data present was sufficient. The crashes might have been due to high traffic, but I haven't confirmed it).<p>\nIn the earlier implementation, the ground truth data contained information about only one bounding box, which meant only one prediction per image (reference listed in the README). For me, it also reduced the training signal and the model was overfitting. So I changed the outputs to a prediction for each default box (as it should be, from what I understood from the SSD paper). Although the initial implementation is good for the purposes for understanding the model.\n\nComments mentioned throughout the code mention what needs to change if the model inputs or outputs are changed.",
      "metadata": {
        "id": "wbsreEfjnQtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Import libraries",
      "metadata": {
        "id": "TdL2AL8yAi00"
      }
    },
    {
      "cell_type": "code",
      "source": "!pip install keras==2.8 bottleneck",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boJRi96WEyXj",
        "outputId": "8f614e73-9919-453c-aadf-8b71dc1f6e44"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n\nRequirement already satisfied: keras==2.8 in /usr/local/lib/python3.7/dist-packages (2.8.0)\n\nRequirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (1.3.5)\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bottleneck) (1.21.6)\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "import os\nimport numpy as np\nfrom tensorflow.keras import layers\nfrom tensorflow import keras \nimport tensorflow as tf\nimport numpy.matlib\nfrom PIL import Image\nfrom tensorflow.keras import backend as K\nfrom scipy.special import softmax\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport bottleneck",
      "metadata": {
        "id": "_hvSY3X-AeFP",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "!python --version",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-1RqrqfT9YD",
        "outputId": "c83f9796-ba17-4fa7-c864-064a01ea700c",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "tf.compat.v1.disable_eager_execution()",
      "metadata": {
        "id": "XAB3KQfSaTHy",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Define Bottleneck Residual layer for MobileNet<br>\nUsing the same parameters as mentioned in the paper",
      "metadata": {
        "id": "EFmrlFJ8uKCe"
      }
    },
    {
      "cell_type": "code",
      "source": "class Bottleneck(keras.Model):\n  def __init__(\n      self,\n      expansion,\n      stride,\n      block_id,\n      filters,\n      alpha=1,\n      ):\n    super(Bottleneck,self).__init__(name = \"Bottleneck_\" + block_id)\n    self.stride = stride\n    self.expansion = expansion\n    self.alpha = alpha\n    self.output_channels = self.alpha * filters\n    self.out = None # there was some problem with eager execution\n\n    prefix =  'Bottleneck_{}_'.format(block_id)\n    self.prefix = prefix\n    # expansion\n    self.expand_BN = layers.BatchNormalization(name = prefix + 'expand_BN')\n    self.expand_ReLU = layers.ReLU(max_value=6, name = prefix + 'expand_ReLU')\n\n    #conv\n    self.Conv = layers.DepthwiseConv2D(\n        kernel_size = 3,\n        padding='same',\n        strides = self.stride,\n        use_bias = False,\n        name = prefix + 'conv')\n    self.Conv_BN = layers.BatchNormalization(name = prefix + 'conv_BN')\n    self.Conv_ReLU = layers.ReLU(max_value=6, name = prefix + 'conv_ReLU')\n\n    #project\n    self.project = layers.Conv2D(\n        filters = self.output_channels,\n        kernel_size = 1,\n        use_bias = False,\n        name = 'contract')\n    self.project_BN = layers.BatchNormalization(name = prefix + 'contract_BN')\n\n    # dimensions need to be the same for residual connection\n    self.residual = layers.Add(name=prefix + 'residual')\n  \n  def build(self, input_shape):\n    self.d = input_shape[-1]\n    \n    self.expand = layers.Conv2D(\n        filters = self.expansion*self.d,\n        kernel_size = 1,\n        use_bias = False,\n        name = self.prefix+'expand')\n\n      \n  def call(self, inputs):\n\n    x = self.expand(inputs)\n    x = self.expand_BN(x)\n    x = self.expand_ReLU(x)\n    self.out = x\n    \n    x = self.Conv(x)\n    x = self.Conv_BN(x)\n    x = self.Conv_ReLU(x)\n\n    x = self.project(x)\n    x = self.project_BN(x)\n\n    if self.output_channels == self.d and self.stride == 1:\n      x = self.residual([inputs,x])\n\n    return x\n\n  def model(self):\n      x = keras.Input(shape=(28,28,3))\n      return keras.Model(inputs=[x], outputs=self.call(x))",
      "metadata": {
        "id": "xmwhyyS7CFyK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Define MobileNetv2<br>\nSame components as mentioned in the paper (the input image dimensions are a bit different)\n",
      "metadata": {
        "id": "cfMYYpWpuQj4"
      }
    },
    {
      "cell_type": "code",
      "source": "#using the architecture mentioned in the paper\nclass MobileNetv2(keras.Model):\n  def __init__(self, k = 11):\n    super(MobileNetv2,self).__init__()\n    self.conv_inp = layers.Conv2D(\n        filters = 32,\n        kernel_size = 3,\n        strides = (2,2),\n        padding='valid',\n        use_bias = False,\n        name = 'conv'\n    )\n    self.k = k    \n\n    self.pad = layers.ZeroPadding2D(padding=2,name='pad')\n    self.BN = layers.BatchNormalization(name='BN')\n    self.ReLU = layers.ReLU(max_value = 6, name = 'ReLU')\n    \n    self.B1_1 = Bottleneck(expansion = 1, filters = 16, stride = 1, block_id = 'B1_1')\n\n    self.B2_1 = Bottleneck(expansion = 6, filters = 24, stride = 2, block_id = 'B2_1')\n    self.B2_2 = Bottleneck(expansion = 6, filters = 24, stride = 1, block_id = 'B2_2')\n\n    self.B3_1 = Bottleneck(expansion = 6, filters = 32, stride = 2, block_id = 'B3_1')\n    self.B3_2 = Bottleneck(expansion = 6, filters = 32, stride = 1, block_id = 'B3_2')\n    self.B3_3 = Bottleneck(expansion = 6, filters = 32, stride = 1, block_id = 'B3_3')\n\n    self.B4_1 = Bottleneck(expansion = 6, filters = 64, stride = 2, block_id = 'B4_1')\n    self.B4_2 = Bottleneck(expansion = 6, filters = 64, stride = 1, block_id = 'B4_2')\n    self.B4_3 = Bottleneck(expansion = 6, filters = 64, stride = 1, block_id = 'B4_3')\n    self.B4_4 = Bottleneck(expansion = 6, filters = 64, stride = 1, block_id = 'B4_4')\n\n    self.B5_1 = Bottleneck(expansion = 6, filters = 96, stride = 1, block_id = 'B5_1')\n    self.B5_2 = Bottleneck(expansion = 6, filters = 96, stride = 1, block_id = 'B5_2')\n    self.B5_3 = Bottleneck(expansion = 6, filters = 96, stride = 1, block_id = 'B5_3')\n\n    self.B6_1 = Bottleneck(expansion = 6, filters = 160, stride = 2, block_id = 'B6_1')\n    self.B6_2 = Bottleneck(expansion = 6, filters = 160, stride = 1, block_id = 'B6_2')\n    self.B6_3 = Bottleneck(expansion = 6, filters = 160, stride = 1, block_id = 'B6_3')\n\n    self.B7_1 = Bottleneck(expansion = 6, filters = 320, stride = 1, block_id = 'B7_1')\n\n    self.conv_out = layers.Conv2D(\n        filters = 1280,\n        kernel_size = 1,\n        strides = (1,1),\n        use_bias = False,\n        name = 'conv_out'\n    )\n    self.avgpool = layers.AveragePooling2D(\n        pool_size = (7,7),\n        name='avg_pool'\n        )\n    \n    self.conv_seg = layers.Conv2D(\n        filters = self.k,\n        kernel_size = 1,\n        strides = (1,1),\n        use_bias = False,\n        name = 'conv_seg'\n    )\n\n  def call(self, inputs):\n    x = self.conv_inp(inputs)\n    x = self.BN(x)\n    x = self.ReLU(x)\n\n    x = self.B1_1(x)\n    x = self.B2_1(x)\n    x = self.B2_2(x)\n\n    x = self.B3_1(x)\n    x = self.B3_2(x)\n    x = self.B3_3(x)\n    \n    x = self.B4_1(x)\n    x = self.B4_2(x)\n    x = self.B4_3(x)\n    x = self.B4_4(x)\n    \n    x = self.B5_1(x)\n    x = self.B5_2(x)\n    x = self.B5_3(x)\n    \n    x = self.B6_1(x)\n    x = self.B6_2(x)\n    x = self.B6_3(x)\n    \n    x = self.B7_1(x)\n\n    x = self.conv_out(x)\n    x = self.avgpool(x)\n    c4 = self.conv_seg(x)\n\n    return c4\n\n  def model(self):\n      x = keras.Input(shape=(224,224,3))\n\n      return keras.Model(inputs=x, outputs=self.call(x))",
      "metadata": {
        "id": "VYGagWE8T2Et"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "MobileNetv2().model().summary()",
      "metadata": {
        "id": "05i7363EYwmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b258607-b136-4fa1-82bd-6547eb179d66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n\nInstructions for updating:\n\nColocations handled automatically by placer.\n\nModel: \"model\"\n\n_________________________________________________________________\n\n Layer (type)                Output Shape              Param #   \n\n=================================================================\n\n input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n\n                                                                 \n\n conv (Conv2D)               (None, 111, 111, 32)      864       \n\n                                                                 \n\n BN (BatchNormalization)     (None, 111, 111, 32)      128       \n\n                                                                 \n\n ReLU (ReLU)                 (None, 111, 111, 32)      0         \n\n                                                                 \n\n Bottleneck_B1_1 (Bottleneck  (None, 111, 111, 16)     2144      \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B2_1 (Bottleneck  (None, 56, 56, 24)       5568      \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B2_2 (Bottleneck  (None, 56, 56, 24)       9456      \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B3_1 (Bottleneck  (None, 28, 28, 32)       10640     \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B3_2 (Bottleneck  (None, 28, 28, 32)       15680     \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B3_3 (Bottleneck  (None, 28, 28, 32)       15680     \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B4_1 (Bottleneck  (None, 14, 14, 64)       21952     \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B4_2 (Bottleneck  (None, 14, 14, 64)       55936     \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B4_3 (Bottleneck  (None, 14, 14, 64)       55936     \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B4_4 (Bottleneck  (None, 14, 14, 64)       55936     \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B5_1 (Bottleneck  (None, 14, 14, 96)       68352     \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B5_2 (Bottleneck  (None, 14, 14, 96)       120768    \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B5_3 (Bottleneck  (None, 14, 14, 96)       120768    \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B6_1 (Bottleneck  (None, 7, 7, 160)        157888    \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B6_2 (Bottleneck  (None, 7, 7, 160)        324160    \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B6_3 (Bottleneck  (None, 7, 7, 160)        324160    \n\n )                                                               \n\n                                                                 \n\n Bottleneck_B7_1 (Bottleneck  (None, 7, 7, 320)        478400    \n\n )                                                               \n\n                                                                 \n\n conv_out (Conv2D)           (None, 7, 7, 1280)        409600    \n\n                                                                 \n\n avg_pool (AveragePooling2D)  (None, 1, 1, 1280)       0         \n\n                                                                 \n\n conv_seg (Conv2D)           (None, 1, 1, 11)          14080     \n\n                                                                 \n\n=================================================================\n\nTotal params: 2,268,096\n\nTrainable params: 2,236,480\n\nNon-trainable params: 31,616\n\n_________________________________________________________________\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Defining SSD<br>\nThe default number of boxes per layer and resolution of each layer is different, since we are working with MNIST data and 224x224 image sizes.<p>\nTo change the number of boxes per layer and layerWidths, some constraints need to be kept in mind which are mentioned in the later sections",
      "metadata": {
        "id": "II03ZRu17FnE"
      }
    },
    {
      "cell_type": "code",
      "source": "class SSD(keras.Model):\n  def __init__(self, numBoxes=[4,6,6,6,4,4], layerWidth=[28,14,7,4,2,1], k = 10+1+4):\n    super(SSD,self).__init__()\n    self.classes = k\n    self.featureMaps = 6\n    self.MobileNet = MobileNetv2(k=k)\n\n    # mark bottleneck_6_1 onwards as non trainable\n    for layer in self.MobileNet.layers[-7:]:\n      layer.trainable=False\n    \n    # For bottleneck_5_3, mark layers beyond conv as non runnable\n    # layers in bottleneck_5_3: ['Bottleneck_B5_3_expand_BN', 'Bottleneck_B5_3_expand_ReLU', 'Bottleneck_B5_3_conv', 'Bottleneck_B5_3_conv_BN', \n    # 'Bottleneck_B5_3_conv_ReLU', 'contract', 'Bottleneck_B5_3_contract_BN', 'Bottleneck_B5_3_residual', 'Bottleneck_B5_3_expand']\n    for layer in self.MobileNet.layers[-8].layers[2:-1]:\n      layer.trainable=False\n\n    self.numBoxes = numBoxes\n    self.layerWidth = layerWidth\n    self.features = [None for _ in range(self.featureMaps)]\n    self.classifiers = [None for _ in range(self.featureMaps)]\n    \n    self.conv1_1 = layers.Conv2D(256,1,name='SSD_conv_1_1')\n    self.conv1_2 = layers.Conv2D(512,3,strides=(2,2),padding='same',name='SSD_conv_1_2')\n\n    self.conv2_1 = layers.Conv2D(128,1,name='SSD_conv_2_1')\n    self.conv2_2 = layers.Conv2D(256,3,strides=(2,2),padding='same',name='SSD_conv_2_2')\n    \n    self.conv3_1 = layers.Conv2D(128,1,name='SSD_conv_3_1')\n    self.conv3_2 = layers.Conv2D(256,3,strides=(1,1),name='SSD_conv_3_2')\n    \n    self.conv4_1 = layers.Conv2D(128,1,name='SSD_conv_4_1')\n    self.conv4_2 = layers.Conv2D(256,2,strides=(1,1),name='SSD_conv_4_2') # changed the kernel size to 2 since the output of the previous layer has width 3\n\n    self.conv = []\n    self.reshape = []\n    for i in range(self.featureMaps):\n      self.conv.append(layers.Conv2D(self.numBoxes[i]*self.classes,3,padding='same',name='Classification_'+str(i)))\n      self.reshape.append(layers.Reshape((self.layerWidth[i]* self.layerWidth[i] * self.numBoxes[i],self.classes),name='Reshape_classification_'+str(i)))\n\n  def build(self, input_shape):\n    self.MobileNet.build(input_shape)\n  \n  def call(self,inputs):\n    x = inputs\n    x = self.MobileNet(x)\n\n    # get the convolved images at different resolutions\n    self.features[0] = self.MobileNet.get_layer('Bottleneck_B4_1').out\n    self.features[1] = self.MobileNet.get_layer('Bottleneck_B5_3').out\n    self.features[2] = self.conv1_2(self.conv1_1(self.features[1]))\n    self.features[3] = self.conv2_2(self.conv2_1(self.features[2]))\n    self.features[4] = self.conv3_2(self.conv3_1(self.features[3]))\n    self.features[5] = self.conv4_2(self.conv4_1(self.features[4]))\n\n    for i in range(self.featureMaps):\n    # for each feature map, create predictions according to the number of boxes for that layer and the number of output channels\n      x = self.conv[i](self.features[i])\n      x = self.reshape[i](x)\n      self.classifiers[i] = x\n    \n    # concatenate all the classifiers\n    x = layers.concatenate(self.classifiers, axis = -2, name='concatenate')\n    return x\n\n\n  def model(self):\n      x = keras.Input(shape=(224,224,3))\n\n      return keras.Model(inputs=x, outputs=self.call(x))",
      "metadata": {
        "id": "aYD2gfR9O8L0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "NUM_CLASSES = 10\n# the first 2 dimensions should be equal to width of the output from the bottleneck expand ReLU at the (4,1) and (5,3) respectively.\n# the dimensions after the second one are determined by the convolutions written inside the SSD (conv1_2, conv2_2, conv3_3, conv4_2)\nlayerWidths = [28,14,7,4,2,1]\nnumBoxes = [3,3,3,3,3,3]\nassert len(numBoxes) == len(layerWidths) # numBoxes for each layer and each layer has a specific width\noutputChannels = NUM_CLASSES + 1 + 4 # 10 classes + background + cx,cy,h,w\nassert outputChannels - NUM_CLASSES == 5",
      "metadata": {
        "id": "29A_FW-GxK4t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "model = SSD(numBoxes=numBoxes, layerWidth=layerWidths, k = outputChannels)\nmodel.model().summary()",
      "metadata": {
        "id": "FmOfPVIjCkuP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea47026c-bdfd-4e2a-96f2-aded2e931d0c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model: \"model_1\"\n\n__________________________________________________________________________________________________\n\n Layer (type)                   Output Shape         Param #     Connected to                     \n\n==================================================================================================\n\n input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n\n                                )]                                                                \n\n                                                                                                  \n\n conv (Conv2D)                  (None, 111, 111, 32  864         ['input_2[0][0]']                \n\n                                )                                                                 \n\n                                                                                                  \n\n BN (BatchNormalization)        (None, 111, 111, 32  128         ['conv[0][0]']                   \n\n                                )                                                                 \n\n                                                                                                  \n\n ReLU (ReLU)                    (None, 111, 111, 32  0           ['BN[0][0]']                     \n\n                                )                                                                 \n\n                                                                                                  \n\n Bottleneck_B1_1 (Bottleneck)   (None, 111, 111, 16  2144        ['ReLU[0][0]']                   \n\n                                )                                                                 \n\n                                                                                                  \n\n Bottleneck_B2_1 (Bottleneck)   (None, 56, 56, 24)   5568        ['Bottleneck_B1_1[0][0]']        \n\n                                                                                                  \n\n Bottleneck_B2_2 (Bottleneck)   (None, 56, 56, 24)   9456        ['Bottleneck_B2_1[0][0]']        \n\n                                                                                                  \n\n Bottleneck_B3_1 (Bottleneck)   (None, 28, 28, 32)   10640       ['Bottleneck_B2_2[0][0]']        \n\n                                                                                                  \n\n Bottleneck_B3_2 (Bottleneck)   (None, 28, 28, 32)   15680       ['Bottleneck_B3_1[0][0]']        \n\n                                                                                                  \n\n Bottleneck_B3_3 (Bottleneck)   (None, 28, 28, 32)   15680       ['Bottleneck_B3_2[0][0]']        \n\n                                                                                                  \n\n Bottleneck_B4_1 (Bottleneck)   (None, 14, 14, 64)   21952       ['Bottleneck_B3_3[0][0]']        \n\n                                                                                                  \n\n Bottleneck_B4_2 (Bottleneck)   (None, 14, 14, 64)   55936       ['Bottleneck_B4_1[0][0]']        \n\n                                                                                                  \n\n Bottleneck_B4_3 (Bottleneck)   (None, 14, 14, 64)   55936       ['Bottleneck_B4_2[0][0]']        \n\n                                                                                                  \n\n Bottleneck_B4_4 (Bottleneck)   (None, 14, 14, 64)   55936       ['Bottleneck_B4_3[0][0]']        \n\n                                                                                                  \n\n Bottleneck_B5_1 (Bottleneck)   (None, 14, 14, 96)   68352       ['Bottleneck_B4_4[0][0]']        \n\n                                                                                                  \n\n Bottleneck_B5_2 (Bottleneck)   (None, 14, 14, 96)   120768      ['Bottleneck_B5_1[0][0]']        \n\n                                                                                                  \n\n Bottleneck_B5_3_expand (Conv2D  (None, 14, 14, 576)  55296      ['Bottleneck_B5_2[0][0]']        \n\n )                                                                                                \n\n                                                                                                  \n\n Bottleneck_B5_3_expand_BN (Bat  (None, 14, 14, 576)  2304       ['Bottleneck_B5_3_expand[0][0]'] \n\n chNormalization)                                                                                 \n\n                                                                                                  \n\n Bottleneck_B5_3_expand_ReLU (R  (None, 14, 14, 576)  0          ['Bottleneck_B5_3_expand_BN[0][0]\n\n eLU)                                                            ']                               \n\n                                                                                                  \n\n SSD_conv_1_1 (Conv2D)          (None, 14, 14, 256)  147712      ['Bottleneck_B5_3_expand_ReLU[0][\n\n                                                                 0]']                             \n\n                                                                                                  \n\n SSD_conv_1_2 (Conv2D)          (None, 7, 7, 512)    1180160     ['SSD_conv_1_1[0][0]']           \n\n                                                                                                  \n\n SSD_conv_2_1 (Conv2D)          (None, 7, 7, 128)    65664       ['SSD_conv_1_2[0][0]']           \n\n                                                                                                  \n\n SSD_conv_2_2 (Conv2D)          (None, 4, 4, 256)    295168      ['SSD_conv_2_1[0][0]']           \n\n                                                                                                  \n\n SSD_conv_3_1 (Conv2D)          (None, 4, 4, 128)    32896       ['SSD_conv_2_2[0][0]']           \n\n                                                                                                  \n\n Bottleneck_B4_1_expand (Conv2D  (None, 28, 28, 192)  6144       ['Bottleneck_B3_3[0][0]']        \n\n )                                                                                                \n\n                                                                                                  \n\n SSD_conv_3_2 (Conv2D)          (None, 2, 2, 256)    295168      ['SSD_conv_3_1[0][0]']           \n\n                                                                                                  \n\n Bottleneck_B4_1_expand_BN (Bat  (None, 28, 28, 192)  768        ['Bottleneck_B4_1_expand[0][0]'] \n\n chNormalization)                                                                                 \n\n                                                                                                  \n\n SSD_conv_4_1 (Conv2D)          (None, 2, 2, 128)    32896       ['SSD_conv_3_2[0][0]']           \n\n                                                                                                  \n\n Bottleneck_B4_1_expand_ReLU (R  (None, 28, 28, 192)  0          ['Bottleneck_B4_1_expand_BN[0][0]\n\n eLU)                                                            ']                               \n\n                                                                                                  \n\n SSD_conv_4_2 (Conv2D)          (None, 1, 1, 256)    131328      ['SSD_conv_4_1[0][0]']           \n\n                                                                                                  \n\n Classification_0 (Conv2D)      (None, 28, 28, 45)   77805       ['Bottleneck_B4_1_expand_ReLU[0][\n\n                                                                 0]']                             \n\n                                                                                                  \n\n Classification_1 (Conv2D)      (None, 14, 14, 45)   233325      ['Bottleneck_B5_3_expand_ReLU[0][\n\n                                                                 0]']                             \n\n                                                                                                  \n\n Classification_2 (Conv2D)      (None, 7, 7, 45)     207405      ['SSD_conv_1_2[0][0]']           \n\n                                                                                                  \n\n Classification_3 (Conv2D)      (None, 4, 4, 45)     103725      ['SSD_conv_2_2[0][0]']           \n\n                                                                                                  \n\n Classification_4 (Conv2D)      (None, 2, 2, 45)     103725      ['SSD_conv_3_2[0][0]']           \n\n                                                                                                  \n\n Classification_5 (Conv2D)      (None, 1, 1, 45)     103725      ['SSD_conv_4_2[0][0]']           \n\n                                                                                                  \n\n Reshape_classification_0 (Resh  (None, 2352, 15)    0           ['Classification_0[0][0]']       \n\n ape)                                                                                             \n\n                                                                                                  \n\n Reshape_classification_1 (Resh  (None, 588, 15)     0           ['Classification_1[0][0]']       \n\n ape)                                                                                             \n\n                                                                                                  \n\n Reshape_classification_2 (Resh  (None, 147, 15)     0           ['Classification_2[0][0]']       \n\n ape)                                                                                             \n\n                                                                                                  \n\n Reshape_classification_3 (Resh  (None, 48, 15)      0           ['Classification_3[0][0]']       \n\n ape)                                                                                             \n\n                                                                                                  \n\n Reshape_classification_4 (Resh  (None, 12, 15)      0           ['Classification_4[0][0]']       \n\n ape)                                                                                             \n\n                                                                                                  \n\n Reshape_classification_5 (Resh  (None, 3, 15)       0           ['Classification_5[0][0]']       \n\n ape)                                                                                             \n\n                                                                                                  \n\n concatenate (Concatenate)      (None, 3150, 15)     0           ['Reshape_classification_0[0][0]'\n\n                                                                 , 'Reshape_classification_1[0][0]\n\n                                                                 ',                               \n\n                                                                  'Reshape_classification_2[0][0]'\n\n                                                                 , 'Reshape_classification_3[0][0]\n\n                                                                 ',                               \n\n                                                                  'Reshape_classification_4[0][0]'\n\n                                                                 , 'Reshape_classification_5[0][0]\n\n                                                                 ']                               \n\n                                                                                                  \n\n==================================================================================================\n\nTotal params: 3,507,342\n\nTrainable params: 3,492,494\n\nNon-trainable params: 14,848\n\n__________________________________________________________________________________________________\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Creating boxes and IoU",
      "metadata": {
        "id": "4RA7NxfQ7_G6"
      }
    },
    {
      "cell_type": "code",
      "source": "# I have used less varying custom scales and aspect ratios here, since the dataset is already uniform\n#IMPORTANT: before changing the scales and aspect ratios, read the comment below\n\n# number of scales is equal to the number of different resolutions ie num of layer widths\n# for a given resolution, we have different aspect ratios\n# num(scales) = num(layerWidth) = num(numBoxes) and num(asp_ratios) = numBoxes[i]\nMinScale = .1 # Min and Max scale given as percentage\nMaxScale = 1.5\nscales = [ MinScale + x/len(layerWidths) * (MaxScale-MinScale) for x in range(len(layerWidths)) ]\nscales = scales[::-1] # reversing the order because the layerWidths go from high to low (lower to higher resoltuion)\n\nasp = [0.5,1.0,1.5]\nasp1 = [x**0.5 for x in asp]\nasp2 = [1/x for x in asp1]",
      "metadata": {
        "id": "yRYi7Ez7UzpH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "IMG_SIZE = 224",
      "metadata": {
        "id": "RwR_TIbzYCix"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# should be equal to the 1st dimension in the output layer of the SSD model\nBOXES = sum([a*a*b for a,b in zip(layerWidths,numBoxes)])\ncentres = np.zeros((BOXES,2))\nhw = np.zeros((BOXES,2))\nboxes = np.zeros((BOXES,4))\nprint(BOXES)",
      "metadata": {
        "id": "wA_IhnyrUl4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "febc1419-72d1-459b-b82f-8c53b802ca2d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3150\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# calculating the default box centres and height, width\nidx = 0\n\nfor gridSize, numBox, scale in zip(layerWidths,numBoxes,scales):\n  step_size = IMG_SIZE*1.0/gridSize\n  for i in range(gridSize):\n    for j in range(gridSize):\n      pos = idx + (i*gridSize+j) * numBox\n      # centre is the same for all aspect ratios(=numBox)\n      centres[ pos : pos + numBox , :] = i*step_size + step_size/2, j*step_size + step_size/2\n      # height and width vary according to the scale and aspect ratio\n      # zip asepct ratios and then scale them by the scaling factor\n      hw[ pos : pos + numBox , :] = np.multiply(gridSize*scale, np.squeeze(np.dstack([asp1,asp2]),axis=0))[:numBox,:]\n\n  idx += gridSize*gridSize*numBox ",
      "metadata": {
        "id": "9A1xGMrXVX18"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# (x,y) co-ordinates of top left and bottom right\n# This actually is not used anywhere. centres[] and hw[] are a good enough substitute\nboxes[:,0] = centres[:,0] - hw[:,0]/2\nboxes[:,1] = centres[:,1] - hw[:,1]/2\nboxes[:,2] = centres[:,0] + hw[:,0]/2\nboxes[:,3] = centres[:,1] + hw[:,1]/2",
      "metadata": {
        "id": "Xp9CrasJhGXI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# calculate IoU for a set of search boxes and default boxes\ndef IoU(box1, box2):\n  box1 = box1.astype(np.float64)\n  box2 = box2.astype(np.float64)\n  # find the left and right co-ordinates of the edges. Min should be less than Max for non zero overlap\n  xmin = np.maximum(box1[:,0],box2[:,0])\n  xmax = np.minimum(box1[:,2],box2[:,2])\n  ymin = np.maximum(box1[:,1],box2[:,1])\n  ymax = np.minimum(box1[:,3],box2[:,3])\n\n  intersection = np.abs(np.maximum(xmax-xmin,0) * np.maximum(ymax-ymin,0))\n  boxArea1 = np.abs((box1[:,2] - box1[:,0]) * (box1[:,3] - box1[:,1]))\n  boxArea2 = np.abs((box2[:,2] - box2[:,0]) * (box2[:,3] - box2[:,1]))\n  unionArea = boxArea1 + boxArea2 - intersection\n  assert (unionArea > 0).all()\n  iou = intersection / unionArea\n\n  return iou",
      "metadata": {
        "id": "TJSIPHPMh3N2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# give the index of the box correpsonding to the IoUs > threshold (=0.5) \nTHRESHOLD = 0.5\ndef bestIoU(searchBox):\n  return np.argwhere(IoU(numpy.matlib.repmat(searchBox,BOXES,1), boxes) > THRESHOLD)",
      "metadata": {
        "id": "iOcpxxIQipbA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Loading Data",
      "metadata": {
        "id": "mm2k4c5Ik_BX"
      }
    },
    {
      "cell_type": "code",
      "source": "TRAINSIZE = 600\nTESTSIZE = 100",
      "metadata": {
        "id": "h07BGB7-k9te"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\nx_train = x_train[:TRAINSIZE , : , :]\ny_train = y_train[:TRAINSIZE]\nx_test = x_test[:TESTSIZE , : , :]\ny_test = y_test[:TESTSIZE]",
      "metadata": {
        "id": "wkZPTKgGq08N"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# take mnist x and y pairs and convert to input, output pairs for the MobileNetv2+SSD model\ndef convert(x,y):\n  MNIST_SIZE = x.shape[-1]\n  # create a 2D array of top left corners for the mnist image to be placed\n  corner = np.random.randint(IMG_SIZE - MNIST_SIZE, size=(x.shape[0],2))\n\n  # create a blank canvas for the input with the required dimension\n  input = np.zeros((x.shape[0], IMG_SIZE, IMG_SIZE, 3))\n\n  # replacing a part by RGB version of MNIST\n  for i in range(x.shape[0]):\n    lx = int(corner[i,0])\n    ly = int(corner[i,1])\n    input[i,lx:lx + MNIST_SIZE, ly:ly+MNIST_SIZE,:] = np.repeat(np.expand_dims(np.array(x[i,:,:]),axis=-1),3,axis=-1)\n\n  # for each default box, there are 5 values: class number and delta cx,cy,h,w\n  output = np.zeros((y.shape[0],BOXES,1+4))\n  output[:,:,0] = NUM_CLASSES # defaulting class labels for all boxes to background initially\n  for i in range(x.shape[0]):\n    bbox = np.zeros(4)\n    bbox[:2] = corner[i]\n    bbox[2:] = corner[i] + (MNIST_SIZE,MNIST_SIZE)\n    # for all default boxes which have IoU > threshold, set the delta values and class number\n    box_idx = bestIoU(bbox).astype(np.uint16)\n    output[i,box_idx,0] = y[i]\n    output[i,box_idx,1] = (bbox[0] + bbox[2])/2.0 - centres[box_idx,0]\n    output[i,box_idx,2] = (bbox[1] + bbox[3])/2.0 - centres[box_idx,1]\n    output[i,box_idx,3] = MNIST_SIZE - hw[box_idx,0]\n    output[i,box_idx,4] = MNIST_SIZE - hw[box_idx,1]\n\n  return input, output\n",
      "metadata": {
        "id": "PEgx1Sdcq7yJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "test_x, test_y = convert(x_test,y_test)\ntrain_x, train_y = convert(x_train,y_train)",
      "metadata": {
        "id": "Sk_z17wV3Bj5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# checking if the inputs prepared are correct or not\nr = np.random.randint(0,train_x.shape[0])\nimg = train_x[r,:,:,:].copy()\nimg_y = train_y[r]\n\nim = np.array(Image.fromarray(img.astype(np.uint8)))\nfig,ax = plt.subplots(1)\nax.imshow(im)\n\n# find all boxes where class label is not background\nidx = np.argwhere(img_y[:,0] != NUM_CLASSES)[:,0]\nprint('Number of boxes with IoU > threshold (0.5):',idx.shape[0])\nprint('Green box: ground truth. Red box: default boxes with IoU < threshold (0.5)')\n\n#calculating the ground truth bounding boxes\ngt = np.zeros(4,dtype=np.uint16)\ngt[:2] = (img_y[idx[0],1:3] + centres[idx[0],:2])\ngt[2:] = (img_y[idx[0],3:] + hw[idx[0],:])\n\n# for some reason, x and y are inverted\nrect = patches.Rectangle((gt[1]-gt[3]/2,gt[0]-gt[2]/2),gt[3],gt[2],linewidth=5,edgecolor='g',facecolor='none')\nax.add_patch(rect)\n\n# showing all the boxes with IoU > 0.5\nfor i in idx:\n  rect = patches.Rectangle((centres[i][1]-hw[i,1]/2,centres[i][0]-hw[i,0]/2),hw[i,1],hw[i,0],linewidth=1,edgecolor='r',facecolor='none')\n  ax.add_patch(rect)\n\nplt.show()",
      "metadata": {
        "id": "NAwnJnu4qE0P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "efca18f0-03bb-4845-d6c0-9780a380b62f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of boxes with IoU > threshold (0.5): 6\n\nGreen box: ground truth. Red box: default boxes with IoU < threshold (0.5)\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))\nprint(train_dataset.element_spec)\nprint(test_dataset.element_spec)",
      "metadata": {
        "id": "1nI7mXjS8zA9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43eae49-6b30-467a-b855-8c21cc3cef64"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(TensorSpec(shape=(224, 224, 3), dtype=tf.float64, name=None), TensorSpec(shape=(3150, 5), dtype=tf.float64, name=None))\n\n(TensorSpec(shape=(224, 224, 3), dtype=tf.float64, name=None), TensorSpec(shape=(3150, 5), dtype=tf.float64, name=None))\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "BATCH_SIZE = 10\nSHUFFLE_BUFFER_SIZE = 60\n\ntrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\ntest_dataset = test_dataset.batch(BATCH_SIZE,drop_remainder=True)",
      "metadata": {
        "id": "oyX8dnwQ8_1k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "LOSS FUNCTION<br>\nHard negative mining hasn't been done here<br>\nInitial idea was to assign weights to background classes, but there is some problem in that approach",
      "metadata": {
        "id": "3EeBg_g29GLU"
      }
    },
    {
      "cell_type": "code",
      "source": "# label is not required here in the standard implementation\n# calculate the smooth L1 loss\ndef smoothL1(x,y,label):\n  diff = K.abs(x-y) #* K.switch(label == 10, label*1.0/BOXES, label)\n  result = K.switch(diff < 1, 0.5 * diff**2, diff - 0.5)\n  return K.mean(result)",
      "metadata": {
        "id": "rMEpljzd9CxT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def confidenceLoss(y,label):\n  unweighted_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(label, y)\n  # class_weights = tf.constant([[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0/BOXES]]*BOXES])\n  # weights = tf.reduce_sum(class_weights * y, axis = -1)\n  # weighted_loss = unweighted_loss * weights\n  return K.mean(unweighted_loss)",
      "metadata": {
        "id": "8fSUhh8O_DsK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def Loss(gt,y):\n  # shape of y is n * BOXES * output_channels\n  # shape of gt is n * BOXES * 5 \n  loss = 0\n  # localisation loss\n  loss += smoothL1(y[:,:,-4:],gt[:,:,-4:],gt[:,:,0:1])\n  # confidence loss\n  loss += confidenceLoss(y[:,:,:-4],tf.cast(gt[:,:,0],tf.int32))\n  return loss",
      "metadata": {
        "id": "gn0xh6OX_BvN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "base_learning_rate = 0.001\n\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate),loss=Loss)",
      "metadata": {
        "id": "A8_O3V8DB_Gk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "history = model.fit(train_dataset,\n                    epochs=25,\n                    validation_data = test_dataset)",
      "metadata": {
        "id": "Z7pp6Fp9DDWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39ca0340-fe39-4016-da7d-1ca95148010b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch 1/25\n\nWARNING:tensorflow:Layer ssd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\n\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\n\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n\n\nWARNING:tensorflow:Gradients do not exist for variables ['mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B5_3/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_expand/kernel:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_2/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_expand/kernel:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_3/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_expand/kernel:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B7_1/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_expand/kernel:0', 'mobile_netv2_1/conv_out/kernel:0', 'mobile_netv2_1/conv_seg/kernel:0'] when minimizing the loss.\n\nWARNING:tensorflow:Gradients do not exist for variables ['mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B5_3/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_expand/kernel:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_2/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_expand/kernel:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_3/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_expand/kernel:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B7_1/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_expand/kernel:0', 'mobile_netv2_1/conv_out/kernel:0', 'mobile_netv2_1/conv_seg/kernel:0'] when minimizing the loss.\n\nWARNING:tensorflow:Gradients do not exist for variables ['mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B5_3/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_expand/kernel:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_2/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_expand/kernel:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_3/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_expand/kernel:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B7_1/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_expand/kernel:0', 'mobile_netv2_1/conv_out/kernel:0', 'mobile_netv2_1/conv_seg/kernel:0'] when minimizing the loss.\n\nWARNING:tensorflow:Gradients do not exist for variables ['mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B5_3/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B5_3/Bottleneck_B5_3_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_1/Bottleneck_B6_1_expand/kernel:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_2/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_2/Bottleneck_B6_2_expand/kernel:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_3/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B6_3/Bottleneck_B6_3_expand/kernel:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_expand_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_expand_BN/beta:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_conv/depthwise_kernel:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_conv_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_conv_BN/beta:0', 'mobile_netv2_1/Bottleneck_B7_1/contract/kernel:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_contract_BN/gamma:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_contract_BN/beta:0', 'mobile_netv2_1/Bottleneck_B7_1/Bottleneck_B7_1_expand/kernel:0', 'mobile_netv2_1/conv_out/kernel:0', 'mobile_netv2_1/conv_seg/kernel:0'] when minimizing the loss.\n\n60/60 [==============================] - 11s 181ms/step - loss: 0.3415 - val_loss: 0.2681\n\nEpoch 2/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0890 - val_loss: 0.0674\n\nEpoch 3/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0528 - val_loss: 0.0564\n\nEpoch 4/25\n\n60/60 [==============================] - 10s 166ms/step - loss: 0.0457 - val_loss: 0.0455\n\nEpoch 5/25\n\n60/60 [==============================] - 10s 166ms/step - loss: 0.0764 - val_loss: 0.0467\n\nEpoch 6/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0215 - val_loss: 0.0264\n\nEpoch 7/25\n\n60/60 [==============================] - 10s 166ms/step - loss: 0.0241 - val_loss: 0.0191\n\nEpoch 8/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0384 - val_loss: 0.0180\n\nEpoch 9/25\n\n60/60 [==============================] - 10s 166ms/step - loss: 0.0250 - val_loss: 0.0160\n\nEpoch 10/25\n\n60/60 [==============================] - 10s 166ms/step - loss: 0.0333 - val_loss: 0.0145\n\nEpoch 11/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0143 - val_loss: 0.0134\n\nEpoch 12/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0207 - val_loss: 0.0144\n\nEpoch 13/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0164 - val_loss: 0.0134\n\nEpoch 14/25\n\n60/60 [==============================] - 10s 166ms/step - loss: 0.0162 - val_loss: 0.0130\n\nEpoch 15/25\n\n60/60 [==============================] - 10s 166ms/step - loss: 0.0135 - val_loss: 0.0124\n\nEpoch 16/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0111 - val_loss: 0.0113\n\nEpoch 17/25\n\n60/60 [==============================] - 10s 166ms/step - loss: 0.0126 - val_loss: 0.0107\n\nEpoch 18/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0115 - val_loss: 0.0110\n\nEpoch 19/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0125 - val_loss: 0.0104\n\nEpoch 20/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0086 - val_loss: 0.0102\n\nEpoch 21/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0137 - val_loss: 0.0104\n\nEpoch 22/25\n\n60/60 [==============================] - 10s 166ms/step - loss: 0.0114 - val_loss: 0.0149\n\nEpoch 23/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0076 - val_loss: 0.0103\n\nEpoch 24/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0078 - val_loss: 0.0093\n\nEpoch 25/25\n\n60/60 [==============================] - 10s 165ms/step - loss: 0.0092 - val_loss: 0.0098\n",
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "model.evaluate(test_x,test_y)",
      "metadata": {
        "id": "g_n2VfMsg1NT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e20fc8d4-968c-4272-9c62-57eeefcee1b3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "4/4 [==============================] - 1s 145ms/step - loss: 0.0098\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": [
              "0.009824990294873714"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "INFERENCE",
      "metadata": {
        "id": "0oNuY-45SngR",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": "# create some sample data\nX, Y = convert(x_test, y_test)",
      "metadata": {
        "id": "VTfYjsyJTEtv",
        "colab_type": "code",
        "colab": {}
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# get prediction for one sample\ny_pred = model.predict(X)\ny_pred.shape",
      "metadata": {
        "id": "QPazH1zFTnE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "535d81f5-7835-47ce-98a0-6d78bfe13c78"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": [
              "(100, 3150, 15)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "OBJperCLASS = 10 # get the top 10 results for each class\n# get the confidence scores (with class values) and delta for the boxes. For each class, the top 10 values are used\ndef infer(Y):\n  # classes are actually the index into the default boxes\n  classes = np.zeros((OBJperCLASS,outputChannels-4),dtype=np.uint16)\n  conf = np.zeros((OBJperCLASS,outputChannels-4))\n  delta = np.zeros((OBJperCLASS,outputChannels-4,4))\n  class_predictions = softmax(Y[:,:outputChannels-4],axis=1)\n  for i in range(outputChannels-4):\n    classes[:,i] = bottleneck.argpartition(class_predictions[:,i],BOXES-1-10,axis=-1)[-OBJperCLASS:]\n    conf[:,i] = class_predictions[classes[:,i],i]\n    delta[:,i] = Y[classes[:,i],outputChannels-4:]\n  return conf,classes, delta\n\n# generate bounding boxes from the inferred outputs\ndef Bbox(confidence,box_idx,delta):\n  #delta contains delta(cx,cy,h,w)\n  bbox_centre = np.zeros((OBJperCLASS,outputChannels-4,2))\n  bbox_hw = np.zeros((OBJperCLASS,outputChannels-4,2))\n  for i in range(OBJperCLASS):\n    bbox_centre[i,:,0] = centres[box_idx[i]][:,0]+delta[i,:,0]\n    bbox_centre[i,:,1] = centres[box_idx[i]][:,1]+delta[i,:,1]\n    bbox_hw[i,:,0] = hw[box_idx[i]][:,0] + delta[i,:,2]\n    bbox_hw[i,:,1] = hw[box_idx[i]][:,1]+delta[i,:,3]\n  return bbox_centre,bbox_hw",
      "metadata": {
        "id": "jrD03dgjcZMO",
        "colab_type": "code",
        "colab": {}
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "r = np.random.randint(TESTSIZE)\n\n# top 10 predictions for each class\nconfidence, box_idx, delta = infer(y_pred[r])\nbbox_centre,bbox_hw = Bbox(confidence, box_idx, delta)\n\nim = np.array(Image.fromarray(X[r].astype(np.uint8)))\nfig,ax = plt.subplots(1)\nax.imshow(im)\n\nfor i in range(outputChannels-4):\n  # skipping backgrounds\n  if i == NUM_CLASSES:\n    continue\n  color = 'r'\n  # if a class is mentioned in the ground truth, color the boxes green\n  if i in Y[r,:,0]:\n    color = 'g'\n    print(i)\n  \n  # skip all the classes which have low confidence values\n  if (confidence[:,i] > 0.5).any() or i in Y[r,:,0]:\n    for k in range(OBJperCLASS):\n      print(\"{}: Confidence-{}\\t\\tCentre-{} Height,Width-{}\".format(i,confidence[k,i],bbox_centre[k,i],bbox_hw[k,i]))\n      \n      # draw bounding box only if confidence scores are high\n      if confidence[k,i] < 0.5:\n        continue\n      x = bbox_centre[k,i,0] - bbox_hw[k,i,0]/2\n      y = bbox_centre[k,i,1] - bbox_hw[k,i,1]/2\n      rect = patches.Rectangle((y,x),bbox_hw[k,i,1],bbox_hw[k,i,0],linewidth=1,edgecolor=color,facecolor='none')\n      ax.add_patch(rect)\n\nplt.show()",
      "metadata": {
        "id": "LY7SOlpafX51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "eef5e296-27f6-46f1-bfeb-422573f1d1e1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": "7\n\n7: Confidence-0.7365235686302185\t\tCentre-[42.19794345 98.16143656] Height,Width-[32.29747125 32.31169936]\n\n7: Confidence-0.8857401013374329\t\tCentre-[31.7246573  91.92364167] Height,Width-[38.88492426 28.39619501]\n\n7: Confidence-0.9466167688369751\t\tCentre-[37.35820007 95.4094162 ] Height,Width-[28.65339181 29.04177281]\n\n7: Confidence-0.9956559538841248\t\tCentre-[37.51727843 93.68477857] Height,Width-[27.14049515 28.02397426]\n\n7: Confidence-0.9956580996513367\t\tCentre-[36.07167462 96.26651478] Height,Width-[27.35519641 40.82926547]\n\n7: Confidence-0.9860585927963257\t\tCentre-[36.14206849 88.0132165 ] Height,Width-[26.96784037 42.68980633]\n\n7: Confidence-0.9980512857437134\t\tCentre-[35.96905352 92.87503022] Height,Width-[27.35304659 40.9342879 ]\n\n7: Confidence-0.9995020627975464\t\tCentre-[39.372262   93.71296859] Height,Width-[28.78150412 28.83308884]\n\n7: Confidence-0.9984629154205322\t\tCentre-[38.21918631 94.18664432] Height,Width-[27.11899134 27.2651076 ]\n\n7: Confidence-0.9972821474075317\t\tCentre-[37.71326017 92.93149531] Height,Width-[29.38378605 28.26766874]\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "id": "Z32HPrzihVqN",
        "colab_type": "code",
        "colab": {}
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}